{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Missing Authentication header or invalid API key', 'code': 401}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('server/.env.dev')\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "response = requests.post(\n",
    "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "  headers={\n",
    "    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "    \"X-Title\": f\"Fitness\", # Optional. Shows in rankings on openrouter.ai.\n",
    "  },\n",
    "  data=json.dumps({\n",
    "    \"model\": \"meta-llama/llama-3.1-8b-instruct:free\", # Optional\n",
    "    \"messages\": [\n",
    "      { \"role\": \"user\", \"content\": \"What is the meaning of life?\" }\n",
    "    ]\n",
    "  })\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "\n",
    "load_dotenv('server/.env.dev')\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "class LLAMA2LLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"claude2\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {OPENROUTER_API_KEY}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        data = {\n",
    "            'model': \"meta-llama/llama-3.1-8b-instruct:free\",\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "        }\n",
    "        response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, data=json.dumps(data))\n",
    "        output = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"n\": self.n}\n",
    "\n",
    "llm = LLAMA2LLM(n=1)\n",
    "\n",
    "def respond(survey):\n",
    "    response_schemas = [\n",
    "        ResponseSchema(\n",
    "            name=\"Goal\",\n",
    "            description=f\"The gym goal is '{survey['What is the gym goal']}'. Generate a schedule that aligns with this goal while considering the user's class schedule.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"FreeTime\",\n",
    "            description=f\"The user has '{survey['How much free time you have']}' each week. Consider this when generating the exercise schedule.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"ClassSchedule\",\n",
    "            description=f\"The user's weekly class schedule is '{survey['What is your weekly schedule looks like']}'. Ensure the exercise schedule does not conflict with this.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"Weight\",\n",
    "            description=f\"The user's current weight is '{survey['What is your current weight?']}'. Take this into account when generating the exercise schedule.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"Height\",\n",
    "            description=f\"The user's height is '{survey['What is your height?']}'. Consider this when generating the exercise schedule.\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\"Generate a schedule with gym exercises for 7 days considering the user's goals, free time, class \"\n",
    "                  \"schedule, weight, height, wakeup time, and meal times.\\n\"\n",
    "                  \"Ensure that the output is in the following JSON format:\\n\"\n",
    "                  \"{format_instructions}\\n\\n\"\n",
    "                  \"User data: {user_data}\"),\n",
    "        input_variables=[\"user_data\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    user_data = {\n",
    "        \"gym_goal\": \"Lose weight\",\n",
    "        \"free_time\": \"10 hours per week\",\n",
    "        \"class_schedule\": {\n",
    "            \"Monday\": \"9:00 AM - 10:30 AM (Math), 2:00 PM - 3:30 PM (History)\",\n",
    "            \"Tuesday\": \"10:00 AM - 11:30 AM (Physics), 1:00 PM - 2:30 PM (Chemistry)\",\n",
    "            \"Wednesday\": \"9:00 AM - 10:30 AM (Math), 2:00 PM - 3:30 PM (History)\",\n",
    "            \"Thursday\": \"10:00 AM - 11:30 AM (Physics), 1:00 PM - 2:30 PM (Chemistry)\",\n",
    "            \"Friday\": \"9:00 AM - 10:30 AM (English), 2:00 PM - 3:30 PM (Biology)\",\n",
    "        },\n",
    "        \"weight\": \"70 kg\",\n",
    "        \"height\": \"170 cm\",\n",
    "    }\n",
    "\n",
    "    response = chain.invoke({\"user_data\": user_data})\n",
    "\n",
    "    return response\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAI, HarmBlockThreshold, HarmCategory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "import getpass\n",
    "\n",
    "\n",
    "# Load the .env.dev file\n",
    "load_dotenv('server/.env.dev')\n",
    "\n",
    "# Access the environment variable\n",
    "key = os.getenv('GOOGLE_API_KEY')\n",
    "hf_api_token=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "\n",
    "# If the Hugging Face token is not set, prompt the user to input it\n",
    "if not hf_api_token:\n",
    "    hf_api_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
    "\n",
    "\n",
    "def load_llm():\n",
    "    llm = GoogleGenerativeAI(\n",
    "        model=\"gemini-pro\", google_api_key=key,\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        },\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def load_llm_from_hf():\n",
    "    # Initialize the LLM using the Hugging Face Endpoint\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        api_key=hf_api_token  # Pass the API token here\n",
    "    )\n",
    "\n",
    "    chat_model = ChatHuggingFace(llm=llm)\n",
    "    \n",
    "    return chat_model\n",
    "\n",
    "\n",
    "\n",
    "def respond(survey):\n",
    "    response_schemas = [\n",
    "        ResponseSchema(\n",
    "            name=\"Goal\",\n",
    "            description=f\"The gym goal is '{survey['What is the gym goal']}'. Generate a schedule that aligns with this goal while considering the user's class schedule.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"FreeTime\",\n",
    "            description=f\"The user has '{survey['How much free time you have']}' each week. Consider this when generating the exercise schedule.\"\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name=\"ClassSchedule\",\n",
    "            description=f\"The user's weekly class schedule is '{survey['What is your weekly schedule looks like']}'. Ensure the exercise schedule does not conflict with this.\"\n",
    "        ),\n",
    "       \n",
    "    ]\n",
    "    \n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Generate a workout schedule for the next 7 days considering the user's goals, free time, class schedule, weight, and height.\\n{format_instructions}\\n{user_data}\",\n",
    "        input_variables=[\"user_data\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "    \n",
    "    llm = load_llm()\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    # Prepare user data to be passed into the chain\n",
    "    user_data = {\n",
    "        \"gym_goal\": survey['What is the gym goal'],\n",
    "        \"free_time\": survey['How much free time you have'],\n",
    "        \"class_schedule\": survey['What is your weekly schedule looks like'],\n",
    "        \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = chain.invoke({\"user_data\": user_data})\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # Handle missing keys or unexpected format here\n",
    "        response = {\"error\": \"The LLM response was incomplete. Please check the input data and try again.\"}\n",
    "    \n",
    "    return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
